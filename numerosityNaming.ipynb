{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RKChHgMVWDU"
      },
      "source": [
        "# ViLT model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we need to install the Transformers library (optionally, also Accelerate):"
      ],
      "metadata": {
        "id": "u0Zgi3vPHDnY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VIV6Q0_BK_I7"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SpoV2sq02-63"
      },
      "outputs": [],
      "source": [
        "from transformers import ViltProcessor, ViltForQuestionAnswering\n",
        "import requests\n",
        "from PIL import Image\n",
        "import math\n",
        "import os\n",
        "import numpy as np\n",
        "import shutil\n",
        "import pandas as pd\n",
        "\n",
        "# prepare ViLT model:\n",
        "processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n",
        "model = ViltForQuestionAnswering.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n",
        "\n",
        "# define paths to the directories:\n",
        "data_path = 'numerosity_naming_images/'\n",
        "categories = ['apples', 'butterflies', 'dots', 'people', 'fastcards']\n",
        "n_categories = len(categories)\n",
        "\n",
        "# Get a list of all the filenames in this directory\n",
        "filenames = os.listdir(data_path)\n",
        "\n",
        "# Loop through the filenames\n",
        "i = 0\n",
        "for filename in filenames:\n",
        "\n",
        "  # prepare target question:\n",
        "  question = \"How many things are there?\" # NB: this might be replaced with a category-specific question\n",
        "\n",
        "  # prepare input for the VQA model\n",
        "  image = Image.open(data_path + filename).convert('RGB')\n",
        "  encoding = processor(image, question, return_tensors=\"pt\")\n",
        "\n",
        "  # forward pass\n",
        "  outputs = model(**encoding)\n",
        "  logits = outputs.logits\n",
        "  idx = logits.argmax(-1).item()\n",
        "  response = int(model.config.id2label[idx])\n",
        "\n",
        "  if response < 0 or response > 20 or math.isnan(response):\n",
        "    print(filename)\n",
        "    print('Unexpected response:', response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vP8QwxwEVn8u"
      },
      "source": [
        "# BLIP-2 model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3BE664OBLrFf"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoProcessor, Blip2ForConditionalGeneration\n",
        "import torch\n",
        "import requests\n",
        "from PIL import Image\n",
        "import math\n",
        "import os\n",
        "import numpy as np\n",
        "import shutil\n",
        "import pandas as pd\n",
        "import re\n",
        "from word2number import w2n\n",
        "\n",
        "# prepare BLIP-2 model:\n",
        "model_names = [\"blip2-opt-6.7b\", \"blip2-flan-t5-xl\", \"blip2-flan-t5-xl-coco\"]\n",
        "processor = AutoProcessor.from_pretrained(\"Salesforce/\" + model_names[1])\n",
        "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/\" + model_names[1], torch_dtype=torch.float16)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "print(device)\n",
        "\n",
        "# define paths to the directories:\n",
        "root_path = 'numerosity_naming_images/'\n",
        "categories = ['apples', 'butterflies', 'dots', 'people', 'fastcards']\n",
        "n_categories = len(categories)\n",
        "\n",
        "# Get a list of all the filenames in this directory\n",
        "filenames = os.listdir(data_path)\n",
        "\n",
        "# Loop through the filenames\n",
        "i = 0\n",
        "for filename in filenames:\n",
        "\n",
        "  # prepare target question:\n",
        "  question = \"How many things are there?\" # NB: this might be replaced with a category-specific question\n",
        "\n",
        "  # prepare input for the VQA model\n",
        "  image = Image.open(data_path + filename).convert('RGB')\n",
        "  encoding = processor(image, text=question, return_tensors=\"pt\").to(device, torch.float16)\n",
        "\n",
        "  # forward pass\n",
        "  generated_ids = model.generate(**encoding, max_new_tokens=10)\n",
        "  response = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
        "\n",
        "  response_num = re.findall(r'\\d+', response)\n",
        "  if not response_num:\n",
        "    try:\n",
        "      response_num = w2n.word_to_num(response)\n",
        "    except ValueError:\n",
        "      print(filename)\n",
        "      print('Unexpected response: ', response)\n",
        "\n",
        "  elif len(response_num) > 1:\n",
        "    print(filename)\n",
        "    print('More than one number returned: ', response)\n",
        "  else:\n",
        "    response_num = response_num[0]\n",
        "\n",
        "  i = i + 1\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}